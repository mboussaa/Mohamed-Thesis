In this chapter, we first summarize all the contributions of this thesis, recalling the challenges and how we addressed each of them. Next and finally, we discuss some perspectives for future work.

\section{Summary of contributions}
Generative software development has paved the way for the creation of multiple generators that serve as a basis for automatically generating code to a broad range of software and hardware platforms. With full automatic code generation, users are able to rapidly synthesize software artifacts for various software platforms. In addition, they can easily customize the generated code for the target hardware platform since modern generators (\ie, C compilers) become highly configurable, offering numerous configuration options that the user can apply. 
The quality of the automatically generated software is highly correlated to the configuration settings as well as the generator itself.
Therefore, we have highlighted, throughout this thesis, the challenges that we face when testing and auto-tuning generators. 

In reviewing the state of the art, we identified numerous approaches for testing generators. However, few of them evaluate the non-functional properties of automatically generated code, namely the performance and resource usage properties. The main issue we have identified when testing the non-functional properties is the oracle problem, since there is no a clear definition of how the oracle might be defined when it comes to the test of the performance and resource usage properties. Similarly, research in auto-tuning generators, especially compilers, has been studied for decades, proposing different solutions for exploring the large optimization search space. However, they  do not exploit the recent advances in search-based software engineering to effectively find the best configuration set. 
 
%in order to provide a new integrated software engineering approach which enables the advanced exploitation of the different dimensions of software diversity. 

From a software engineering point of view, this thesis contributes to improve the quality and reliability of generators. The contributions are summarized as follows: 

Our first contribution addresses the problem of non-functional testing of generators. In particular, we tackle the oracle problem in the domain of code generators testing. Thus, we propose an approach for automatically detecting inconsistencies in code generators in terms of non-functional properties (\ie, resource usage and performance).
Our approach is based on the intuition that a code generator is often a member of a family of code generators. Therefore, we benefit from the existence of multiple generators with comparable functionality (\ie, code generator families) to apply the idea of metamorphic testing~\cite{zhou2004metamorphic}, defining high-level test oracles (\ie, metamorphic relations) as test oracles. 
%To do so,  since we are comparing equivalent implementations of the same program written in different languages, we assume that the memory usage and execution time should be more or less the same with a small variation for each test suite across the different versions.
We define the metamorphic relation as a comparison between the variations of performance and resource usage of code, generated from the same code generator family. Any variation that exceeds a specific threshold value is automatically detected as an anomaly. We apply two statistical methods (\ie, principal component analysis and range-charts) in order to automate the inconsistencies detection.
We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. We evaluate the properties related to the resource usage and performance for five different target software platforms. 
We run a bench of test suites across 7 Haxe benchmark libraries in order to verify the metamorphic relation (\ie, the performance and memory usage variation) for each of them. Experimental results show that our approach is able to detect, among 95 executed test suites, 11 performance and 15 memory usage inconsistencies, violating the metamorphic relation. These results show that our approach can automatically detect real issues in code generator families.

The second contribution addresses the problem of generators auto-tuning. Particularly, we are interested in auto-tuning compilers because of the large number of configuration options (\ie, optimizations) they offer to control the quality of the generated code. In this context, we exploit the recent advances in search-based software engineering in order to provide an effective approach to tune compilers (\ie, through optimizations) according to user's non-functional requirements (\ie, performance and resource usage). Our approach, called NOTICE, applies a novel formulation, compared to previous related work, of the compiler
optimization problem using the Novelty Search algorithm\cite{lehman2008exploiting}. Novelty Search is applied to tackle the
problem of optimizations diversity and then, providing a new way to explore the huge optimization search space. In fact, since the search
space of possible combinations is multi-modal\cite{bodin1998iterative} and too large, we apply this technique is to explore the search space of possible compiler optimization options by considering sequence diversity as a single objective.
We conduct an empirical study to evaluate the effectiveness of our approach by verifying the optimizations performed by the GCC compiler. Our experimental results show that NOTICE is able to auto-tune compilers according to user choices (heuristics, objectives, programs, etc.) and construct optimizations that yield to better performance results than standard optimization levels and classical genetic algorithms. We also demonstrate that NOTICE can be used to automatically construct optimization levels that represent optimal trade-offs between the speedup and memory usage using multi-objective algorithms.

Evaluating the resource usage of automatically generated code is complex because of the diversity of software and hardware platforms that exist in the market. To handle this problem, we present in the third contribution the technical details of the infrastructure used to collect the non-functional metrics (e.g., memory and CPU consumptions) of automatically generated code (by either compilers or code generators). In fact, we benefit from the recent advances in lightweight system virtualization, in particular container-based virtualization, in order to offer effective support for automatically deploying, executing, and monitoring the generated code in heterogeneous environment.
The same monitoring infrastructure is used to evaluate the experiments conducted in the two first contributions.

\section{Perspectives}
The work presented in this thesis represents a step towards proving support to evaluate generators. In the reminder of this chapter we will describe possible improvements and extensions to the contributions of this thesis.

\paragraph{Tracking the source of code generator inconsistencies} 
In Chapter \ref{chap:code generators}, we present a black-box testing approach that identifies the presence of potential issues within code generator families. However, we do not provide detailed information about the source of the issues. We investigated the generated code manually in order to fix the bug. As a future work, we believe that a traceability method can be applied to collect and visualize information about the inconsistency, at the source code level. Thus, we can help code generator maintainers to easily identify the source of the bugs (\eg, code snippets that affect the the software performance), and fixing the issues.

\paragraph{Improving the efficiency of our auto-tuning approach}
We plan to explore more trade-offs among resource usage metrics \eg, the correlation between CPU consumption and speedup. Using the Docker technology, we can run containers easily on different host machines in the cloud. As a future work, we want to evaluate our approach across different hardware architectures.
We also intend to provide more facilities to NOTICE users in order to test optimizations performed by modern compilers such as Clang, LLVM, etc.
Finally, NOTICE can be easily adapted and integrated to new case studies. As an example, we would inspect the behavior of code generators since different optimizations can be performed to generate code from models~\cite{stuermer2007systematic}. The same approach can be applied as long as the code generator accept many configuration options.

\paragraph{Speed up the time required to tune and test generators}
Throughout the experiments conducted in this thesis, we use to run containers sequentially. This can take too much time, especially when configuring compilers the tuning time grows exponentially as long as we have new configurations to evaluate. 
In order to reduce the time needed to run and monitor multiple versions of generated code (e.g., optimized versions), we intend to deploy tests on many nodes in the cloud using multiple containers in parallel. Doing so, we will be able to accelerate the testing process. Solutions as Docker Swarm already exist and can be applied to manage clusters of containers running in parallel in the cloud.

\paragraph{Interact with generator experts in order to improve our testing approach}
Few months after running our experiments, the Haxe community has released a new version of the PHP code generator\footnote{\url{https://github.com/HaxeFoundation/haxe/releases/tag/3.4.0}} with many performance improvements, especially for arrays. As we expected, they introduced advanced functions (\textit{array\_fill}\footnote{\url{https://github.com/HaxeFoundation/haxe/blob/f375ec955b41550546e494e9f79a5deefa1b96ac/std/php7/Global.hx\#L226}}) for initiating arrays in order improve the performance. Actually, we are discussing with the Haxe community in order to expand our testing approach, introducing new target software platforms to test and creating new benchmark programs with large number of test suites. Finally, we should evaluate the impact of the new code generator improvements (\ie, running the same experiments with new code generator versions) and check if the fixes have eliminated the identified inconsistencies.


