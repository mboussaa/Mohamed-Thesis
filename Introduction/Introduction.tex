

\section{Context}
Modern software systems rely nowadays on a highly heterogeneous and dynamic interconnection of platforms and devices that provide a wide diversity of capabilities and services. These heterogeneous services may run in different environments ranging from cloud servers with virtually unlimited resources down to resource-constrained devices with only a few KB of RAM. Effectively developing software artifacts for multiple target platforms and hardware technologies is then becoming increasingly important. As a consequence, we observe in the last years~\cite{Czarnecki:2000:GPM:345203}, that high-level abstract development received more and more attraction to tame with the runtime heterogeneity of platforms and technological stacks that exist in several domains such as mobile or Internet of Things~\cite{betz2011improving}.
Therefore, software developers tend to increasingly use generative programming~\cite{Czarnecki:2000:GPM:345203} and model-based techniques~\cite{france2007model} in order to reduce the effort of software development and maintenance by developing at a higher-level of abstraction through the use of domain-specific languages for example. 
Consequently, the new advances in hardware and platform specifications have paved the way for the creation of multiple code generators and compilers that serve as a basis to target different ranges of software platforms and hardware. 
On the one hand, code generators are needed to transform the high-level system specifications (e.g., textual or graphical modeling language) into conventional source code programs (e.g., general-purpose languages such as Java, C++, etc). The generated code has no details about the hardware platform on which the generated machine code will run. For example, the generated application can be executed in a Microsoft Windows environment as a C\# application that interacts with an SQL Server database; or in a Linux environment as a Java application interacting with a MySQL database. 
On the other hand, compilers are needed to bridge this gap by taking into account different hardware architectures and properties such as register usage, memory organizations, hardware-specific optimizations, etc. Compilers are often needed to transform the source code, that was manually or automatically generated, into machine code (i.e., binaries, executables). 


However, code generators as well as compilers are known to be difficult to understand since they involve a set of complex and heterogeneous technologies and configurations whose complex interdependencies pose important challenges. The complexity of code generators makes the activities of design, implementation, and testing very hard and time-consuming\cite{guana2015developers}. 

As a consequence, software testers need to check the generated source code or binaries with almost the same, expensive effort as is needed for manually written code. 
Testing code generators and compilers becomes crucial and necessary to guarantee that no errors are incorporated by inappropriate modeling or by the code generator it self. 
Faulty code generators or compilers can generate defective software artifacts which range from uncompilable or semantically dysfunctional code that causes serious damage to the target platform; to non-functional bugs which lead to poor-quality code that can affect system reliability and performance (e.g., high resource usage, high execution time, etc.). 
Numerous approaches have been proposed\cite{stuermer2007systematic,yang2011finding} to verify the functional outcome of generated code. However, there is a lack of solutions that pay attention to evaluate the non-functional properties of produced code.

In this thesis, we seek to test and evaluate the properties related to the resource usage of generated code in order to find non-functional bugs. 
On the one hand, since many different target software platforms can be targeted by the code generator, we would help code generator creators to monitor the execution of generated code for different targets and have a deep understanding of its non-functional behavior in terms of resource usage. Thus, we would detect bugs and inconsistencies caused by some faulty code generators. 
On the other hand, compilers may have a huge number of optimizations and code transformation rules that can be applied during the code generation process. As a consequence, we would help compiler users to select the best optimizations that satisfy specific resource usage requirements for a broad range of programs and hardware architectures.

\textit{This thesis addresses the problem of non-functional testing of code generators and compilers through the runtime execution and evaluation of the generated code}. In particular, it aims at offering effective support for collecting data about resource consumption (e.g., CPU, memory) and detect bugs leading to a resource-intensive usage, as well as efficient mechanisms to help compiler users to choose the best configuration that satisfy specific non-functional requirements and lead to performance improvement.


\section{Challenges}
Due to new advances in hardware and system specification, creating an effective code generators (including compilers) is not simple and it is becoming more and more challenging.
In existing solutions that aim to test code generators and compilers, we find two important challenges. Addressing these challenges, which are described below, is the objective of the present work.
\begin{itemize}
\item
\textbf{Monitoring code generators/compilers behavior:} For testing the non-functional properties of code generators and compilers, developers generally use to compile, deploy and execute generated software artifacts on different execution platforms. Then, they have to collect and compare information about the performance and efficiency of the generated code. Afterwards, they report issues related to the code generation process such as incorrect typing, memory management leaks, etc.
Currently, there is a lack of automatic solutions to check the performance issues such as the inefficiency (high memory/CPU consumption) of the generated code. In fact, developers often use manually several platform-specific profilers, debuggers, and monitoring tools\cite{guana2014chaintracker,delgado2004taxonomy} in order to find some inconsistencies or bugs during code execution. Ensuring the quality of generated code in this case can refer to several non-functional properties such as code size, resource or energy consumption, execution time, among others\cite{pan2006fast}. Due to the heterogeneity of execution platforms and hardwares, collecting information about the non-functional properties of generated code becomes very hard and time-consuming task since developers have to analyze and verify the generated code for different target platforms using platform-specific tools. 
\item
\textbf{Tuning code generators/compilers:} The current innovations in science and industry demand ever-increasing computing resources while placing strict requirements on many non-functional properties such as system performance, power consumption, size, response, reliability, etc. In order to deliver satisfactory levels of performance on different processor architectures, compiler creators often provide a broad collection of optimizations that can be applied by compiler users in order to improve the quality of generated code. However, to explore the large optimization space, users have to evaluate the effect of optimizations according to a specific performance objective/trade-off. Thus, constructing a good set of optimization levels for a specific system architecture/target application becomes challenging and time-consuming problem. Due to the complex interactions and the unknown effect of optimizations, users find difficulties to choose the adequate compiler configuration that satisfy a specific non-functional requirement.





%while satisfying all the non-functional requirements for a broad range of programs and architectures
 


\end{itemize}
The challenges this research tackle can be summarized in the following research questions. These questions arise from the analysis of the drawbacks in the previous paragraphs.

\textit{RQ1.} How can we help users to automatically choose the adequate compiler/code generator configuration that satisfy specific non-functional requirements?

\textit{RQ2.} How can we automatically detect inconsistencies and non-functional errors within code generators?

\textit{RQ3.} How can we provide efficient support for resource consumption monitoring and management?


\section{Contributions}
This thesis establishes three core contributions in the field of non-functional testing of code generators and compilers. 
These contributions are briefly described in the rest of this section.

\textbf{Contribution: automatic compiler auto-tuning according to the non-functional requirements.}
Compilers generally provide different kinds of optimizations that can be applied during code transformation in order to optimize the produced binaries for a specific hardware architecture. Thus, we present, in this contribution, a new search-based meta-heuristic called Novelty search for compiler optimizations exploration. This approach will help compiler users to automatically auto-tune compilers according to specific non-functional requirements namely resource usage properties. 
We evaluate the effectiveness of our approach by verifying the optimizations performed by the GCC compiler.
Our experimental results show that our approach is able to auto-tune compilers according to user requirements and construct optimizations that yield to better performance results than standard optimization levels. We also demonstrate that our approach can be used to automatically construct optimization levels that represent optimal trade-offs between multiple non-functional properties such as execution time and resource usage requirements.

\textbf{Contribution: automatic detection of inconsistencies within code generators families.}
In this contribution, we propose a new approach for testing and monitoring of code generators families. This approach try to find automatically real issues in existing code generators. 
It is based on the intuition that a code generator is often a member of a family of code generators. The availability of multiple generators with comparable functionality enables to apply the idea of differential testing\cite{mckeeman1998differential} to detect code generator issues.
We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. Experimental results show that our approach is able to detect some performance inconsistencies that reveal real issues in Haxe code generators.
In particular, we show that we could find two kinds of errors during code transformation: the lack of use of a specific function and an abstract type that exist in the standard library of the target language which can reduce the memory usage/execution time of the resulting program.

\textbf{Contribution: a microservice-based infrastructure for runtime deployment and monitoring of generated code.}
We propose a micro-service infrastructure to ensure the deployment and monitoring of different variants of generated code. 
It will also automate the process of code compilation, deployment and execution in order to provide to software developers more facilities to test the generated code. 
This isolated and sand-boxing environment is based on system containers, as execution platforms, to provide a fine-grained understanding and analysis of resource usage in terms of CPU and memory. This approach constitutes the playground for testing and evaluating the generated code from either compilers or code generators. This contribution answers mainly \textit{RQ3} but the same infrastructure is used across all experiments in \textit{RQ1} and \textit{RQ2}.

\section{Overview of this thesis}
The remainder of this thesis is organized  as follows:

\textbf{Chapter 2} first contextualizes this research, situating it in the domain of generative programming. We give a background about the different concepts involved in the field of generative programming as well as an overview of the different aspects of automatic code generation in software development. 


\textbf{Chapter 3}  presents the state of the art regarding our approach. This chapter
provides a survey of the most used techniques for testing compilers and code generators. We focus more on the non-functional testing aspects.
This chapter is divided on two parts. On the one hand, we study the previous approaches that have been applied for compiler auto-tuning. On the other hand, we study the different techniques used to test the functional and non-functional properties of code generators. 

\textbf{Chapter 4} resumes the work done related to compiler testing. To do so, we study the impact of compiler optimizations on the generated code. Thus, we present a search-based technique called Novelty search for compiler optimizations exploration. We provide two adaptations of this algorithm: mono and multi objective search. We also show how this technique can easily help compiler users to efficiently generate and evaluate the compiler optimizations. The non-functional metrics we are evaluating are the performance, memory and CPU usage. We evaluate this approach through an empirical study and we discuss the results.

\textbf{Chapter 5} presents our approach for the non-functional testing of code generators. It shows an adaptation of the idea of differential testing for detecting code generator issues. We report the results of testing multiple generators with comparable functionalities (a code generator family). The non-functional metrics we are evaluating in this section are the performance and memory usage of generated code. We also report the issues we have detected and we propose solutions for code generation improvement.

\textbf{Chapter 6} shows the testing infrastructure used across all experiments. It shows the usefulness of such architecture, based on system containers, to automatically deploy and execute the generated code by either compilers or code generators. We report the comparison results of using this infrastructure and a non-containerized solution in terms of overhead and we discuss the results. 

\textbf{Chapter 7} draws conclusions and identifies future work and perspectives for testing compilers and code generators.

\section{Publications}

\begin{itemize}
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Sunyé, Benoit Baudry:
	\textbf{Automatic Non-functional Testing of Code Generators Families}. In
	\textit{The 15th International Conference on Generative Programming: Concepts \& Experiences (GPCE 2016)},
	Amsterdam, Netherlands, October 2016.

	\item Mohamed Boussaa, Olivier Barais, Benoit Baudry, Gerson Sunyé:
	\textbf{NOTICE: A Framework for Non-functional Testing of Compilers}. In 
	\textit{2016 IEEE International Conference on Software Quality, Reliability \& Security (QRS 2016)}, Vienna, Austria, August 2016.
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Sunyé, Benoit Baudry:
	\textbf{A Novelty Search-based Test Data Generator for Object-oriented Programs}. In 
	\textit{Genetic and Evolutionary Computation Conference Companion (GECCO 2015)}, 
	Madrid, Spain, July 2015.
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Sunyé, Benoit Baudry:
	\textbf{A Novelty Search Approach for Automatic Test Data Generation}. In
	\textit{8th International Workshop on Search-Based Software Testing (SBST@ICSE 2015)}, 
	Florence, Italy, May 2015.

	
	
\end{itemize}


