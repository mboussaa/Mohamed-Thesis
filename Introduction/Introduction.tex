

\section{Context}
Modern software systems rely nowadays on a highly heterogeneous and dynamic interconnection of platforms and devices that provide a wide diversity of capabilities and services. These heterogeneous services may run in different environments ranging from cloud servers with virtually unlimited resources down to resource-constrained devices with only a few KB of RAM. Effectively developing software artifacts for multiple target platforms and hardware technologies is then becoming increasingly important. As a consequence, we observe in the last years~\cite{Czarnecki:2000:GPM:345203}, that generative software development received more and more attraction to tame with the runtime heterogeneity of platforms and technological stacks that exist in several domains such as mobile or Internet of Things~\cite{betz2011improving}.

Generative programming~\cite{czarnecki2000generative} offers a software abstraction layer that software developers can use to specify the desired system behavior (\eg, using domain-specific languages DSLs, models, etc.), and automatically generate software artifacts. As a consequence, the new advances in hardware and platform specifications have paved the way for the creation of multiple \textit{generators} that serve as a basis for automatically generating code to a broad range of software and hardware platforms. 

%software developers tend to increasingly use generative programming~\cite{Czarnecki:2000:GPM:345203} and model-based techniques~\cite{france2007model} in order to reduce the effort of software development and maintenance by developing at a higher-level of abstraction through the use of domain-specific languages (DSLs) for example. ``

Automatic code generation involves two kinds of generators: source code generators and compilers.
On the one hand, code generators are needed to transform the high-level system specifications (\eg, textual or graphical modeling language) into conventional source code programs (\eg, general-purpose languages GPLs such as Java, C++, etc). 
On the other hand, compilers bridge the gap between the input programs (\ie, written using GPLs) and the target execution environment, producing low-level machine code (\ie, binaries, executables) for a specific hardware architecture.
%are needed to transform automatically the high-level source code programs, that were manually written or automatically generated, into low-level machine code (\ie, binaries, executables). 


%For example, compilers bridge the gap between the input programs (\ie, written using GPLs) and the target execution environment by taking into account different hardware architectures and properties such as register usage, memory organizations, hardware-specific optimizations, etc. 

 
%In general, the generated code has no details about the hardware platform on which the generated machine code will run. 
%For example, the generated application can be executed in a Microsoft Windows environment as a C\# application that interacts with an SQL Server database; or in a Linux environment as a Java application interacting with a MySQL database. 
%Automatic code generation can \deleted{of course} improve the quality and consistency of a program as well \added{as} the productivity of software development~\cite{kapteijns2009comparative}.
 
With full automatic code generation, it is now possible to develop the code easily and rapidly, improving the quality and consistency of a program as well the productivity of software development\cite{kapteijns2009comparative}. 
In addition, today's modern generators (\eg, C compilers) become highly configurable, offering
(numerous) configuration options (\eg, optimization passes) to users in order to tune the produced code with respect to the target software and/or hardware platform. 
%Thus, finding a good combination of options that best customize the generated code according to the user requirements is in general a very challenging problem.


In this context, it is crucial that the software being automatically generated undergoes an appropriate testing technique to verify the correct behavior of generators. Hence, users can trust the code generator and gain confidence in its correct operation.  
%Similarly, generators have to be evaluated and examined in order to ensure that the code they produce is correct. 
Verifying the correctness of generated code can be either functional (\eg, verifying that the generated code exhibits the same functional behavior as described in the input program), or non-functional (\eg, verifying the quality of generated code).

 
 

%However, code generators as well as compilers are known to be difficult to evaluate since they involve a set of complex and heterogeneous technologies and configurations whose complex interdependencies pose important testing challenges. Moreover, they become highly configurable which often make the task of choosing the best configuration set very difficult.




\section{Motivation}

%When the software developer intends to apply automatic code production, he may create his own generator (compiler or code generator), introducing some configuration options to the users in order to customize the generated code, or he could benefit from the work of others by using and configuring an existing off-the-shelf generator to automatically produce code for the target software platform or hardware architecture. 

As we stated in the context of this thesis, today's modern generators are highly configurable, letting the user to easily customize the automatically generated code. In the meantime, efficiently testing configurable generators poses important challenges since it is too costly to manually or automatically execute and test all configurations. 
%Nowadays, compilers become more user-friendly and mature than code generators\cite{fursin2008milepost}. This is because they are more used and experienced in software development than code generators. 
For instance, popular generators such as GCC, LLVM, etc., are widely used in software development and they
%do not need to be tested because they have already been (rigorously) evaluated and validated by compiler creators before tool commercialization. Therefore, software developers can directly use them.
%However, compilers such as GNU compilers 
offer a large selection of configuration options to control the generator behavior. 
Different categories of options can be enabled (\ie, option flags) to help developers to: debug, optimize, and tune application performance, select language levels and extensions for compatibility, select the target hardware architecture, and perform many other common tasks that configure the way the code is generated.
%These compiler options can be enabled through a combination of environment variables, compiler configuration files, command line options, and plugins. 
The huge number of generator configurations, versions, optimizations, and debugging utilities make the task of choosing the best configuration set very difficult and time-consuming. As an example, GCC version 4.8.4 provides a wide range of command-line options that can be enabled or disabled by users, including more than 150 options for optimization. This results in a huge design space with $2^{150}$ possible optimization combinations that can be enabled by the user. In addition, constructing one single optimization sequence that improves the code quality for all programs is impossible since the interactions between optimizations is too complex and difficult to define. As well, the optimization's impact is highly dependent on the hardware and the input source code.
%there is no only one set of optimizations that will work for all programs. It highly depends on the hardware architecture and the input program.
This example shows how painful it is for the users to tune generators such as compilers (through optimization flags) in order to satisfy different non-functional properties such as execution time, compilation time, code size, etc.

%Thus, the generated executables can be easily customized to fit with the target hardware seatings.
%Unlike code generators, 

%Source code generators are \replaced{slightly}{less} configurable\deleted{ than compilers}. In fact, code generator users have little control on the behavior and design of code generators which give less freedom to customize/tune the generated code. This is because code generators act generally as a black box where code transformations/optimizations are internally managed in a very complex way. 
%Hence, 
Before tuning generators, it is crucial to test if the code generation works properly. If so, users will trust the tool and will more likely to continue using it for production code generation. Contrarily, any issue with the generated code leads to a loss of confidence in generators and users will unlikely continue to use them during software development. As a consequence, checking the correctness of generated code has to be done with almost the same expensive effort as it is needed for manually written code.
In this context, compared to compiler testing\cite{yang2011finding,le2014compiler}, code generators lack of testing solutions to automatically evaluate their correct behavior, especially for the non-functional properties.
That is because code generators are less used and experienced in industry compared to compilers. In addition, they are difficult to test since they involve a set of complex and heterogeneous technologies that are internally managed in a very complex way\cite{guana2015developers,guana2014chaintracker}.
Testing code generators is principally the tool experts responsibility. Nevertheless, users (\eg, customers) are also responsible of this validation since they will continuously report the faults encountered during code generation. 
%For code generator creators, testing the automatic code generation consists on applying a virtuous cycle known as the \textit{"edit, compile, and test"} cycle. For example, in case of releasing a new generator version, developers edit the templates and transformation rules that define the code generation process to add new rules and settings, and then generate the output files. These output files are then compiled (or not) and the generated application is executed and tested. At this point, if they find a problem in the generated code, they alter the templates or the input of the generator and re-generate. This cycle is repeated as long as new changes are applied. 
%As an example, in model-driven engineering, a platform-dependent model (PIM) is transformed into a platform specific model (PSM). PIM to PSM translations are done either by hand or by applying automatic model transformation tools. Then code generation is performed from PSM by using some sort of template-based code generator. Generated code is then inspected and augmented by developers. In case of errors the program model, code templates, or implementation platform model are adjusted and program code is generated repeatedly\cite{herrington2003code}. This process iterates until results are satisfactory.
%In case of using an off-the-shell code generator during software development (\eg, commercial code generators), 
%users do have control on the structure and design of the input high-level language or model. 
%users (\ie, software developers) generally test code generators by passing well-designed programs supported by the generator (\eg, DSL, Model, GPL, etc) and generate code to the target software platform. In this case, since the generator is not editable, the quality of the generated code depends only on the efficiency of the selected code generator for the target platform. So, testing code generators is mainly to find coding errors at the source code level. If they find any issues with the generated code, the bugs are reported to the generator suppliers in order to fix them (\eg, add documentation, add typing support, etc.).
Faulty code generators can generate defective software artifacts which range from uncompilable or semantically dysfunctional code that causes serious damage to the target platform; to non-functional bugs which lead to poor-quality code that can affect system reliability and performance (\eg, high resource usage, high execution time, etc.).
Numerous approaches have been proposed~\cite{stuermer2007systematic,yang2011finding} to verify the functional outcome of generated code. However, there is a lack of solutions that pay attention to evaluate the properties related to the performance and resource usage of automatically generated code. 

%This is because code generators act generally as a black box where code transformations/optimizations are internally managed in a very complex way. 


In summary, from the user's point of view, generators (compilers and code generators) are black box components that can be used to facilitate the software production process. The quality of the generated software is directly correlated to the quality of the generator itself. As long as the quality of generators is maintained and improved, the quality of generated software artifacts also improves. Any bug with these generators impacts on the software quality delivered to the market and results in a loss of confidence on the end users.
In particular, when automatic code generation is used, we identify two major issues that threaten the quality of generated software:
on the one hand, highly configurable generators control the quality of generated code through numerous optimization options than can be enabled/disabled by users. This huge configuration space poses an important challenge for users to select the best optimization options that meet some non-functional requirements.
On the other hand, the complexity of code generators as well as the lack of solutions for evaluating the non-functional properties of generated code represent an obstacle for the users who need more evidence to continue using them during software development. 



%the huge design space of compiler configuration options as well as  are two major challenges we tackle in this thesis. 

%complexity of code generators make the activities of design, implementation, and testing very hard and time-consuming\cite{guana2015developers}.

%Testing code generators consists, in general, on verifying the behavior of generated code. 
 %Testing code generators and/or correctly tuning compilers is crucial and necessary to guarantee that no errors are incorporated by inappropriate modeling or by the compiler itself.  


%In short, automatically testing and tuning configurable code generators pose many challenges, especially for the non-functional requirements such as the resource usage and execution speed of generated code, etc.
\section{Scope of the thesis}

In this thesis, we seek to evaluate the quality of the generated code in terms of performance and resource usage.
On the one hand, we provide facilities to the code generator creators/maintainers to monitor the execution of generated code for different target software platforms and have a deep understanding of its non-functional behavior in terms of resource usage. Consequently, we automatically detect the non-functional issues caused by some faulty code generators. 
On the other hand, we provide a mechanism that helps users (\ie, software developers) to select the best optimization sets that satisfy specific resource usage or performance requirements for a broad range of programs and hardware architectures.

This thesis addresses three problems: 
	
	(1) \textbf{The problem of non-functional testing of code generators:} We benefit from the existence of multiple code generators with comparable functionality (\ie, code generator families) to automatically test the generated code. We leverage the metamorphic testing\cite{chen1998metamorphic} to detect non-functional inconsistencies in code generator families.
	We focus in this contribution on testing the performance and resource usage properties (\eg, intensive resource usage).
	An inconsistency is detected when the generated code exhibits an unexpected behavior in terms of performance or resource usage compared to all equivalent implementations in the same code generator family.
	%We define the metamorphic relation as a comparison between the variations of performance and resource usage of code, generated from the same code generator family. %We focus in this contribution on testing the performance and resource usage properties (\eg, intensive resource usage).
	
	(2) \textbf{The problem of generators auto-tuning:}  
	We exploit recent advances in search-based software engineering in order to provide an effective approach to effectively tune generators (\eg, GCC compilers) through optimizations. We also demonstrate that our approach can be used to automatically construct optimization levels that represent optimal trade-offs between multiple non-functional properties such as execution time and resource usage requirements. 

	(3) \textbf{The problem of software platforms diversity and hardware heterogeneity in software testing:} Running tests and evaluating the resource usage in heterogeneous environments is tedious. To handle this problem, we benefit from the recent advances in lightweight system virtualization, in particular container-based virtualization, in order to offer effective support for automatically deploying, executing, and monitoring code in heterogeneous environment, and collect non-functional metrics (\eg, memory and CPU consumptions).
	

In the rest of the thesis, we use the term \textbf{``compilers''} to refer to the traditional compilers that take as input a source code and translate it into machine code like GCC, LLVM, etc. Similarly, \textbf{``Code generators''} designate the software programs that transform an input program into source code like Java, C++, etc. Finally, we use the term \textbf{``generators''} to designate both, code generators and compilers. 

\section{Challenges}
%Due to new advances in hardware and system specification, creating an effective code generators (including compilers) is not simple and it is becoming more and more challenging.
In existing solutions that aim to test and auto-tune generators, we find three important challenges. Addressing these challenges, which are described below, is the objective of the present work.
\begin{itemize}
\item
\textbf{Code generators testing: the oracle problem:} One of the most common challenges in software testing is the oracle problem. A test oracle is the mechanism by which a tester can determine whether a program has failed or not.
When it comes to the non-functional testing of code generators, this problem becomes very challenging because it is quite hard to determine the expected output of automatically generated code (\eg, memory consumption). Determining whether these non-functional outputs correspond to a generator anomaly or not is not obvious. That is why testing the generated code becomes very complex when testers have has no precise definition of the oracle they would define. 
To alleviate the test oracle problem, techniques such as metamorphic testing\cite{chen1998metamorphic} are widely used to test programs without defining an explicit oracle. Instead, it employs high-level metamorphic relations to automatically verify the outputs.
So, which kind of test oracles can we define? How can we automatically detect inconsistencies? All these questions pose important challenges in testing code generators.

%@techreport{chen1998metamorphic,
%  title={Metamorphic testing: a new approach for generating next test cases},
%  author={Chen, Tsong Y and Cheung, Shing C and Yiu, Shiu Ming},
%  year={1998},
%  institution={Technical Report HKUST-CS98-01, Department of Computer Science, Hong Kong University of Science and Technology, Hong Kong}
%}

\item
\textbf{Auto-tuning compilers: exploring the large optimizations search space:} The current innovations in science and industry demand ever-increasing computing resources while placing strict requirements on many non-functional properties such as system performance, power consumption, size, reliability, etc. In order to deliver satisfactory levels of performance on different processor architectures, compiler creators often provide a broad collection of optimizations that can be applied by compiler users in order to improve the quality of generated code. However, to explore the large optimization space, users have to evaluate the effect of optimizations according to a specific performance objective/trade-off. Thus, constructing a good set of optimization levels for a specific system architecture/target application becomes very challenging. Moreover, due to the complex interactions and the unknown effect of optimizations, users find difficulties to choose the adequate compiler configuration that satisfies a specific non-functional requirement.

\item
\textbf{Runtime monitoring of generated code: handling the diversity of software and hardware platforms:} To evaluate the properties related to the resource usage of generated code (by compilers or code generators), developers use generally to compile, deploy and execute the generated software artifacts on different execution platforms. Then, after gathering the non-functional metrics, they report issues related to the code generation process such as program crash, abnormal termination, memory leaks, etc.
%Currently, there is a lack of automatic solutions that check the performance issues such as the inefficiency (high memory/CPU consumption) of the generated code. 
In fact, developers often use several platform-specific profilers, debuggers, and monitoring tools~\cite{guana2014chaintracker,delgado2004taxonomy} in order to find some inconsistencies or bugs during code execution. 
%Ensuring the quality of the generated code in this case can refer to several non-functional properties such as code size, resource or energy consumption, execution time, among others~\cite{pan2006fast}. 
Due to the heterogeneity of execution platforms and hardware, collecting information about the resource usage of generated code becomes very hard and time-consuming tasks since developers have to analyze and verify the generated code for different target platforms using platform-specific tools. 

%while satisfying all the non-functional requirements for a broad range of programs and architectures

\end{itemize}
The challenges this research tackle can be summarized in the following research questions. These questions arise from the analysis of the challenges presented in the previous paragraphs.

\textit{RQ1.} How can we help code generator creators/maintainers to automatically detect non-functional inconsistencies in code generators?

\textit{RQ2.} How can we help compiler users to automatically choose the adequate compiler configuration that satisfies a specific non-functional requirement?

\textit{RQ3.} How can we provide efficient support for resource consumption monitoring in heterogeneous environment?


\section{Contributions}
This thesis establishes three core contributions. They are briefly described in the rest of this section.

\textbf{Contribution I: Automatic detection of inconsistencies in code generator families.}
In this contribution, we tackle the oracle problem in the domain of code generators testing.
The availability of multiple generators with comparable functionality (\ie, code generator families) allows us to apply the idea of metamorphic testing~\cite{zhou2004metamorphic} by defining high-level test oracles (\ie, metamorphic relation) to detect issues. We define the metamorphic relation as a comparison between the variations of performance and resource usage of code, generated from the same code generator family.
We evaluate our approach by analyzing the performance of Haxe, a popular high-level programming language that involves a set of cross-platform code generators. We evaluate the properties related to the resource usage and performance for five target software platforms. Experimental results show that our approach is able to detect several performance inconsistencies that reveal real issues in this code generator family. 

\textbf{Contribution II: An approach for auto-tuning compilers.}
As we stated earlier, the huge number of compiler options requires the application of a search method to explore the large design space. Thus, we apply, in this contribution, a search-based meta-heuristic called \textit{Novelty Search}~\cite{lehman2008exploiting} for compiler optimizations exploration. This approach helps compiler users to efficiently explore the large optimization search space and auto-tune compilers according to the performance and resource usage properties and that, for a specific hardware architecture. 
We evaluate the effectiveness of our approach by verifying the optimizations performed by the GCC compiler.
Our experimental results show that our approach is able to auto-tune compilers according to the user requirements and construct optimizations that yield to better performance results than standard optimization levels. We also demonstrate that our approach can be used to automatically construct optimization levels that represent optimal trade-offs between multiple non-functional properties such as execution time and resource usage requirements.

\textbf{Contribution III: A lightweight execution environment for software testing and monitoring.}
Finally, we propose a micro-service infrastructure to ensure the deployment and monitoring of the different variants of generated code. This contribution addresses the problem of software and hardware heterogeneity. Thus, we describe an approach that automates the process of code generation and compilation, execution, and monitoring in order to facilitate the testing and auto-tuning process.
This isolated and sand-boxing environment is based on system containers, as execution platforms, to provide a fine-grained understanding and analysis of the properties related to the resource usage (CPU and memory). 
This approach constitutes the playground for testing and tuning generators. This contribution answers mainly \textit{RQ3} but the same infrastructure is particularly used to validate the carried experiments in \textit{RQ1} and \textit{RQ2}.

\section{Overview of this thesis}
The remainder of this thesis is organized  as follows:

\textbf{Chapter \ref{chap:background}} first contextualizes this research, situating it in the domain of generative programming. We discuss several concepts and stakeholders involved in the field of generative programming as well as an overview of the different aspects of automatic code generation. Finally, we discuss the different problems that make the task of generators auto-tuning and testing very difficult.

\textbf{Chapter \ref{chap:SOTA}} presents the state of the art of the thesis. This chapter provides a survey of the most used techniques for tuning and testing generators.
This chapter is divided into three sections. First, we study the different techniques used to test the functional and non-functional properties of code generators. Second, we study the previous approaches that have been applied for auto-tuning compilers. Finally, we present several research efforts that used the container-based architecture for software testing and monitoring. At the end, we provide a summary of the sate of the art and we discuss several open challenges.

\textbf{Chapter \ref{chap:code generators}} presents our approach for non-functional testing of code generator families. It shows an adaptation of the idea of metamorphic testing to automatically detect code generator issues. We conduct a statistical analysis and we report the experimental results conducted using an example of code generator families. The inconsistencies we detect are related to the performance and memory usage of generated code. We also propose solutions to fix the detected issues.

\textbf{Chapter \ref{chap:compilers}} resumes our contribution for compiler auto-tuning. Thus, we present an iterative process based on a search technique called Novelty Search for compiler optimization exploration. We provide two adaptations of this algorithm: mono and multi-objective optimization. We also show how this technique can easily help compiler users to efficiently generate and evaluate compiler optimizations. The non-functional metrics we are evaluating are the performance, memory, and CPU usage. We evaluate this approach through an empirical study and we discuss the results.

\textbf{Chapter \ref{chap:docker}} shows the testing infrastructure used across all experiments. It shows the usefulness of such architecture, based on system containers, to automatically execute and monitor the automatically generated code. 
%We evaluate our infrastructure by comparing it to a non-containerized solution. We report the comparison results between both solutions in terms of overhead and we discuss the results. We also provide a case study to show the automatic monitoring process. 

\textbf{Chapter \ref{chap:conclusion}} draws conclusions and identifies future directions for testing and auto-tuning generators.
\newpage
\section{Publications}

\begin{itemize}
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Suny\'e, Beno\^it Baudry:
	\textbf{Automatic Non-functional Testing of Code Generators Families}. In
	\textit{The 15th International Conference on Generative Programming: Concepts \& Experiences (GPCE 2016)},
	Amsterdam, Netherlands, October 2016.

	\item Mohamed Boussaa, Olivier Barais, Beno\^it Baudry, Gerson Suny\'e:
	\textbf{NOTICE: A Framework for Non-functional Testing of Compilers}. In 
	\textit{2016 IEEE International Conference on Software Quality, Reliability \& Security (QRS 2016)}, Vienna, Austria, August 2016.
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Suny\'e, Beno\^it Baudry:
	\textbf{A Novelty Search-based Test Data Generator for Object-oriented Programs}. In 
	\textit{Genetic and Evolutionary Computation Conference Companion (GECCO 2015)}, 
	Madrid, Spain, July 2015.
	
	\item Mohamed Boussaa, Olivier Barais, Gerson Suny\'e, Beno\^it Baudry:
	\textbf{A Novelty Search Approach for Automatic Test Data Generation}. In
	\textit{8th International Workshop on Search-Based Software Testing (SBST@ICSE 2015)}, 
	Florence, Italy, May 2015.

	
	
\end{itemize}


