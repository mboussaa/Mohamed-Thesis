 
In this chapter, the context of this thesis and the general problems it faces are introduced. The objective of this chapter is to give a brief introduction to  different domains and concepts in which our work takes place and used throughout this document.
This includes generative programming techniques, an overview of the software development tool chain and the main concepts for testing code generators and compilers auto-tuning.

 %It aims at providing a better understanding of the background and context in which our work takes place, as well as the terminology and concepts presented in the next chapters.

The chapter is structured as follows: 

In section 2.1, we present the problem of software diversity and hardware heterogeneity carried out by the continuous innovation in science and technology.

Section 2.2 aims at providing a better understanding of the generative programming concept that is increasingly applied to ease software development. 

In section 2.3, we present the different steps of automatic code generation involved during software development as well the different stakeholders and their roles in testing generators. We highlight then, the main activities that the software developer goes through from the software design until the release of the final software product.

Section 2.4 gives an overview of the different types of code generators used in the literature and we show the complexity of testing code generators.. 

Similarly, in Section 2.5, we describe some compiler optimizations and we illustrate the compiler auto-tuning complexity by presenting the different challenges that this task is posing.


\section{Diversity in software engineering}
%context
The history of software development shows a continuous increase of complexity in several aspects of the software development process. This complexity is highly correlated with the actual technological advancement in the software industry as more and more heterogeneous devices are introduced in the market~\cite{betz2011improving}. 
Generally, heterogeneity may occur in terms of different system complexities, diverse programming languages and platforms, types of systems, development processes and distribution among development sites\cite{ghazi2015heterogeneous}.
%System heterogeneity we are discussing in this thesis is the software and hardware diversity.
System heterogeneity is often led by software and hardware diversity.
Diversity emerges as a critical concern that spans all activities in software engineering, from design to operation\cite{acher2014software}. It appears in different areas such as mobile and web development\cite{doukas2013compose}, security\cite{allier2015multitier}, etc.

However, software and hardware diversity leads to a greater risk for system failures due to the continuous change in configurations and system specifications.
As a matter of a fact, effectively developing software artifacts for multiple target platforms and hardware technologies is then becoming increasingly important.
Furthermore, the increasing relevance of software in general and the higher demand in quality and performance contribute to the complexity of software development.

In this background introduction, we discuss two different dimensions of diversity: (1) software diversity and (2) hardware heterogeneity.

%The history of software development shows a continuous increase of complexity in several aspects of the software development process~\cite{betz2011improving}. 
%Diversity
 
 
%problem
%Furthermore, the increasing relevance of software in general and the higher demand in quality and performance contribute to the complexity of software development. 
%Today, softwares and services are running everywhere. These services are running on top of heterogeneous software and hardware platforms.
\subsection{Software diversity}
In today's software systems, different software variants are typically developed simultaneously to address a wide range of application contexts and customer requirements\cite{schaefer2012software}. 
Therefore, software is built using different approaches and languages. 

In order to understand the skills and capabilities required to develop softwares on top of different classes of devices, we queried a popular open-source repository \textit{GitHub} to evaluate the heterogeneity of existing programming languages. 

The following sets of keywords were used: \textit{1) Cloud:} server with virtually unlimited resources, \textit{2) Microcontroller:} resource constrained node (few KB RAM, few MHz),  \textit{3) Mobile:} an intermediate node, typically a smartphone,  \textit{4) Internet of Things:} Internet-enabled devices,  \textit{5) Cyber Physical System}, and  \textit{6) Embedded systems}, as a large and important part of the service implementations will run as close as possible to physical world, embedded into sensors, devices and gateways.

\begin{figure}[h]
	\center
	\includegraphics[scale=1.]{Background/fig/github}
	\caption{Popularity of 10 programming languages for the different areas related to software development}
\end{figure}

Figure 2.1 presents the results of those queries. The queried keywords are presented on the x-axis together with the number of matches for that keyword. For each keyword, the y-axis represents the popularity (in per cent of the total number of matches) of each of the 10 most popular programming languages that we encountered.


This simple study indicates that no programming language is popular across the different areas. A general trend indicates that Java and JavaScript (and to some extent, Python and Ruby) are popular in cloud and mobile, whereas C (and to some extent, C++) is a clear choice for developers targeting embedded and microcontroller-based systems. Other languages do not score more 10\% for any of the keywords. 
For all keywords except Cloud , the combined popularity of Java, JavaScript and C/C++ (i.e, the sum
of the percentages) is above 70\%. For Cloud, we observe a large use of Python, Ruby also being very popular, so the combined popularity of Java, JavaScript and C/C++ is only 50\%. It is also worth noticing that the most popular language for a given keyword scores very poorly (less than 5\%) for at least another keyword. While it might appear that a combination of C/C++, JavaScript and Java should be able to cover all the areas, in practice it does not exclude the need for other programming languages. For example, the Fibaro Home Center 2 (a gateway for home automation based on the Z-Wave protocol) uses Lua as scripting language to define automation rules. Another example is the BlueGiga BlueTooth Smart Module, which can be scripted using BGScript, a proprietary scripting language. This shows that each part of an infrastructure might require the use of a niche language, middleware or library to be exploited to its full potential.


This variation of programming languages for the different kinds devices induces a high \textit{software diversity}. 

Baudry et al.\cite{baudry2015multiple} and Schaefer et al.\cite{schaefer2012software} have presented an exhaustive overview of the multiple facets of software diversity in software engineering. 
Software diversity can emerge in different types and dimensions such as diversity of operating systems, programming languages, data structures, components, execution environments, etc. 
Like all modern software systems, softwares have to be adapted to address changing requirements over time supporting system evolution, technology and market needs like considering new software platforms, new languages, new customer choices, etc.
%We discuss in this section some of them which are the most relevant for this thesis.







Accordingly, we propose the following definition of software diversity: 
\textit{\textbf{Software diversity is the generation or implementation of the same program specification in different ways/manners in order to satisfy one or more diversity dimension such as the diversity of programming languages, execution environments, functionalities, etc. }}
		
%We define as well the term \textbf{"software family"} to categorize these diverse programs that share commonalities. 

%The key concept of code generators is to produce code in a general-purpose language, such as Java or C++, that can be compiled and executed. Target execution platforms of the generated code are heterogeneous and diverse.


\subsection{Hardware heterogeneity}
On the hardware side, modern software systems rely nowadays on a highly heterogeneous and dynamic interconnection of devices that provide a wide diversity of capabilities and services to the end users.
These heterogeneous services run in different environments ranging from cloud servers to resource-constrained devices.
Hardware heterogeneity comes from the continuous innovation of hardware technologies to support new system configurations and architectural design (e.g., addition of new features, a change in the processor architecture, new hardware is made available, switch to low bandwidth wireless communication, etc). 
For example, until February 2006\footnote{\url{https://arstechnica.com/information-technology/2016/02/moores-law-really-is-dead-this-time/}}, the increase in capacity of microprocessors has followed the famous Moore's law\footnote{\url{https://en.wikipedia.org/wiki/Moore\%27s_law}} for Intel processors. Indeed, the number of components (transistors) that can be fitted onto a chip doubles every two years, increasing the performance and energy efficiency.
For instance, Intel Core 2 Duo processor was introduced in 2006 with 291 millions of transistors and 2.93 GHz clock speed. Two years later, Intel has introduced the Core 2 Quad processors which came up with 2.66 GHz clock speed and the double number of transistors introduced in 2006 with 582 millions of transistors.

So, given the complexity of new emerging processors architecture (x86, x64, etc) and CPU manufacturers such as ARM, AMD and Intel, some of the questions that developers have to answer when facing hardware heterogeneity: 
Is it easy to deliver satisfactory levels of performance on modern processors? How is it possible to produce machine code that can exploit efficiently the new hardware changes? 

To cope with the heterogeneous hardware platforms, software developers use different compilers (for compiled languages such as C or C++) in order to compile their high-level source code programs and execute them on top of a board range of platforms and processors. 

\begin{figure}[h]
	\center
	\includegraphics[scale=0.65]{Background/fig/compilers}
	\caption{Compiler architecture}
\end{figure}

As shown in Figure 2.2, a compiler is typically divided into two parts, a front end and a back end. The compiler front-end verifies syntax and semantics and analyzes the source code to build an internal representation of the program, called the intermediate representation or IR. For example, the GNU Compiler Collection (GCC) and LLVM support many front ends with languages such as C, C++, Objective-C, Objective-C++, Fortran, Java, Ada, and Go among others. 
A compiler back end is typically responsible for code optimizations and code generation for a particular microprocessor. Today, GCC is able to generate code for approximately \textbf{more than 40 different processor architectures}.
For example, one important option for compiler flags is \textit{-march}. It tells the compiler what code it should produce for the system's processor architecture (or arch); it tells GCC that it should produce code for a certain kind of CPU. Using \textit{-march=native} enables all the optimization flags that are applicable for the native system's CPU, with all its capabilities, features, instruction sets, and so on. There exits many other optimization options for the target CPU like \textit{--with-arch=i7}, \textit{--with-cpu=corei7}, etc.
Generally, each time a new family of processors is released, compiler developers release new compiler version with more sophisticated optimization options for the target platform. For example, old compilers produce only 32-bit programs. These programs still run on new 64-bit computers, but they may not exploit all processor capabilities (e.g. they will not use the new instructions that are offered by x64 CPU architecture). For instance, the current x86-64 assembly language can still perform arithmetic operations on 32-bit registers using instructions like addl, subl, andl, orl, etc, with the l standing for "long", which is 4 bytes/32 bits. 64-bit arthimetic is done with addq, subq, andq, orq, etc, with q standing for "quadword", which is 8 bytes/64 bits.


Today's CPUs are highly parallel processors with different levels of parallelism.
We find parallelism everywhere from the parallel execution units in a CPU core, up to the SIMD
(Single Instruction, Multiple Data) instruction set and the parallel execution of multiple
threads. 

Modern computers can do many things at once. One of the commonly applied code optimizations by modern compilers in parallel computing is vectorization. It constitutes the process of converting an algorithm from a scalar implementation, which processes a single pair of operands at a time, to a vector implementation, which processes one operation on multiple pairs of operands at once (series of adjacent values).

Programmers can exploit vectorization to speedup certain parts of their code. One major research topic in computer science is the search for methods of automatic vectorization: seeking methods that would allow a compiler to convert scalar algorithms into vectorized algorithms without human intervention.

In short, software developers need to deal with these compiler configurations to truly take advantage of the new chip with more advanced optimizations for the new hardware chip.



%Which optimizations are applied by compiler users in order to satisfy  the non-functional properties of a broad range of programs and hardware architectures such as energy consumption, execution time, etc. 


%end 


%Model-Driven Software Engineering and generative programming techniques to provide a new integrated software engineering approach which enables the advanced exploitation of the full range of diversity and specificity of the future computing continuum

%Diversity increases system complexity and leads to a greater risk for system failures. Efficient validation and verification methods are, thus, essential to guarantee qualities of diverse systems, such as security, consistency, correctness or performance

\subsection{Matching software diversity to heterogeneous hardware: the marriage}
The hardware and software communities are both facing significant change and major challenges. Hardware and software are pulling us in opposite directions. Figure 2.3 shows an overview of the challenges that both communities are facing.  

On the one hand, software is facing challenges of a similar magnitude, with major changes in the way software is deployed, is sold, and interacts with hardware. 
Software diversity, as discussed in section 2.1.1, is driven by software innovation, driving the software development toward highly configurable and complex systems. This complexity is carried by the huge number of software technologies, customer configurations, execution environments, programming languages, etc. This explosion of configurations that software is facing makes the activity of testing and validation very difficult and time consuming. 
As a consequence, softwares become higher and higher level, managing complexity and gluing lot of pieces together to give programmers the right abstraction for how things really work and how the data is really represented. 

\begin{figure}[h]
	\center
	\includegraphics[scale=0.65]{Background/fig/marriage}
	\caption{Matching software to hardware}
\end{figure}

On the other hand, hardware is exposing us to more low level details and heterogeneity due to the continuous hardware innovation. 
Hardware innovation offer us energy efficiency, performance improvement but exposes a lot of complexity for software engineers and developers (e.g., compilers users/creators).
For example, in \cite{he2010computer} authors argue that system software is not ready for this heterogeneity and cannot fully benefit from new hardware advances such as multi-core and many-core processors. Although multi-core processors has been used in everyday life, we still do not know how to best organize and use them. 
Meanwhile, hardware specialization for every single application is not a sustainable way of building chips.
%So, what the software community can do to address/deal with devices heterogeneity? How hardware innovation can be exploited in the software?

Matching software to hardware is ensured by providing the adequate software languages and compilers that have to produce efficient code to the target hardware (relation 1 in Figure 2.3). As consequence, people who are writing compilers have to continuously enhance the way the executables are produced by releasing new compiler versions to support new hardware changes (i.e., new optimization flags, instruction sets).

For example, Hou et al.\cite{hou2010spap} have presented SPAP, a container-based programming language for heterogeneous many-core systems. This language allows programmers to write unified programs that are able to run efficiently on heterogeneous processors. SPAP comes with a set of compilers and runtime environments to such hardware processors. Chafi et al.\cite{chafi2010language,chafi2011domain} proposed leveraging domain specific languages (DSLs) to map high-level application code to
heterogeneous devices.
%Results show that the presented DSL can achieve high performance on heterogeneous parallel hardware with no modification required to the source code. They compared this language performance to MATLAB code and they showed that it outperformed it in nearly all cases.

To avoid hardware heterogeneity, software developers use for example managed languages such as JAVA, Scala, C\#, etc to favor software portability. Instead of compiling to native machine instruction set, these languages are compiled into an intermediate language or IL, which is similar to a binary assembly language. These instructions are executed by a JVM, or by .NET's CLR virtual machine, which effectively translates them to native binary instructions specific to the CPU architecture and/or OS of the machine.

By using managed code and compiling in this managed execution environment, memory management such as a garbage collector, type safety checking, and destruction of unneeded objects are handled internally within this sandbox runtime environment. Thus, developers focus on the business logic of applications to provide more secure and stable software without taking too much care of the hardware heterogeneity.

However, using managed languages has drawbacks. It includes slower startup speed (the managed code must be JIT compiled by the VM), the managed code can be slower than native code, and generally increased use of system resources on any machine that is executing the code.

In contrast, devices in turn, may impose the support of specific programming languages (relation 2 in Figure 2.3). 
For example, C language is the most widely used programming language in the context of embedded systems\footnote{\url{http://www.eetimes.com/author.asp?doc_id=1323907}} where the system is really resource-constrained. C utilizes the hardware to its maximum by multi-processing and multi-threading APIs provided by POSIX. It also controls the memory management and uses less memory (which allows more freedom on memory management compared to the use of garbage collector, for example).

In mobile development for example, Java is needed to implement Android applications and Objective-C is needed to develop iOS products. This means that developers need to create multiple clients in this heterogeneous environment. 
%We can see also that C/C++ are the most used languages for targeting embedded systems.

%Well, it's like this. Every language is created with a single target platform in mind ( except haXe of course, it can target multiple platforms). For example, Java is compiled into Java bytecode which is executed in the Java Platform ( Java Runtime Library). C/C++ is used to built  applications executed by the OS. So, the target platform of C/C++ is a specific OS. C# is built for the Microsoft .NET framework. Javascript is built for the web platform.
%But, the problem with this is that each platform has its own language tied to it, so it is very difficult to write for different platforms as you need to program in different languages. Also, it becomes difficult to combine platforms together.

%This is where haXe steps in to show us the way. To quote from the haXe website :
%IF YOU COULD ONLY LEARN ONE PROGRAMMING LANGUAGE, HAXE WOULD BE IT.
%IT'S UNIVERSAL. IT'S POWERFUL. IT'S EASY-TO-USE.

 

%which is big problem for both communities
%we need a marriage between hardware and software



%to handle hardware hetergenouty is parallalism ubiquity and differentiation


 %abstract, choose, and exploit hardware heterogeneity providing computational power at low energy consumption levels.

%For example, although Android provides Java syntax, it uses its own Google libraries and creates byte code that will not run on the standard JVM (Java Virtual Machine). This means that consumers are carrying devices that support different programming languages and developers will usually need to create multiple clients in this heterogeneous environment.



\section{From classical software development to generative programming}
\label{sec:FROM} 
In comparison to the classical approach where software development was carried out manually, today’s modern development requires more automatic and flexible approaches to address software diversity and hardware heterogeneity as described in the previous sections.
Hence, more generic tools, methods and techniques are applied in order to keep the software development process as easy as possible for testing and maintenance and to handle the different requirements in a satisfyingly and efficient manner.
%GP
As a consequence, generative programming (GP) techniques are increasingly applied to automatically generate and reuse software artifacts.
%GP definition
\begin{mydef}[\textbf{Generative programming}]
		Generative programming is a software engineering paradigm based on modeling software families such that, given a particular requirements specification, a highly customized and optimized intermediate or end-product can be automatically manufactured on demand from elementary, reusable implementation components by means of configuration knowledge~\cite{Czarnecki:2000:GPM:345203}.
\end{mydef}

This paradigm offers the promise of moving from "one-of-a-kind" software systems to the semi-automated manufacture of wide diversity of software.

%figure
Generative software engineering consists on using higher-level programming techniques such as meta-programming, modeling, DSL, etc. in order to provide a new integrated software engineering approach which enables the advanced exploitation of the different dimensions of software diversity and automatically generate efficient code for the target software platform. 

In principle, a software development process can be seen as a mapping between a problem space and a solution space~\cite{czarnecki2005overview} (see Figure 2.4). 

%problem space
\textbf{The problem space} is a set of domain-specific abstractions that can be used by application engineers to express their needs and specify the desired system behavior. This space is generally defined  as DSLs or high-level models. 

%solution space
\textbf{The solution space} consists of a set of implementation components, which can be composed to create system implementations (for example, the generation of platform-specific software components written using general-purpose languages such as Java, c++, etc).

%mapping
\textbf{The configuration knowledge} constitutes the mapping between both spaces. It takes a specification as input and returns the corresponding implementation as output. It defines the construction rules (i.e., the translation rules to apply in order to translate the input model/program into specific implementation components) and optimizations (i.e., optimization can be applied during code generation to enhance some of the non-functional properties such as execution speed). It defines also the dependencies and settings among the domain specific concepts and features.

\begin{figure}[h]
	\center
	\includegraphics[scale=0.65]{Background/fig/GDM.pdf}
	\caption{Generative programming concept}
\end{figure}
%GP advantages
These schema integrates several powerful concepts from Model Driven Engineering (MDE), such as domain-specific languages, feature modeling, and code generators.

Some commonly benefits of such software engineering process are:
\begin{itemize}
\item It reduces the amount of re-engineering/maintenance caused by specification requirements
\item It facilitates the reuse of components/parts of the system
\item It increases the decomposition and modularization of the system
\item It handles the heterogeneity of target software platforms by automatically generating code
\end{itemize}


An example of generative programming application is the use of Software Product Lines (SPL).
SPL-based software diversity is often coupled to generative programming techniques\cite{Czarnecki:2000:GPM:345203} that enable the automatic production of source code from variability models. This technique implies the use of automatic code generators to generate code that satisfies user requirements (SPL models).
Schaefer et al.\cite{schaefer2012software} surveys software diversity by means of SPLs. This technique enables one to manage a set of related features to build diverse products in a specific domain. Thus, this solution is able to control software diversity by handling the diversity of requirements such as user requirements or environmental constraints or changes. 

JHipster\footnote{https://jhipster.github.io/} is also another concrete example of generative programming application in industry. JHipster is an application generator based on YO generator which provides tools to generate quickly modern web applications using Java stack on the server side (using Spring Boot) and a responsive Web front-end on the client side (with AngularJS and Bootstrap).
The generated web application can be quite different from one user to another. It really depends on the options/choices selected by the user to build a configured application. The selected parameter values will configure the way the JHipster code generators will produce code. 
For example, Figure 2.5 shows a feature model of some configuration examples that the user would select. When building the applications, the user may select the database type he would generate, the Java version, the network protocol, etc. 
Using this feature model, \textbf{more than 10k diverse architecture types} of project can be selected which means that 10k program variants may be generated depending on the different criteria.

Whatever configuration selected by the user, the application behavior will not change and the generated application will share a similar architecture and fundamental code-base.
\begin{figure}[h]
	\center
	\includegraphics[scale=0.65]{Background/fig/jhipster}
	\caption{Example of JHipster feature model}
\end{figure}



\textbf{Among the main contributions of this thesis is to evaluate the impact of applied configurations during code transformation/optimization (by whether code generators or compilers) on the resource usage requirements. 
}

In the following section, we present a general overview of the complete software development tool chain and the main actors that are involved from design time to runtime.


\section{An overview of the software development tool chain}
The process of generative software development involves many different technologies. In this section, we describe in more details the different activities and stakeholders involved to transform high-level system specifications into executable programs and that from design time to runtime.
\begin{figure}[h]
	\center
	\includegraphics[scale=0.65]{Background/fig/background_overview2.pdf}
	\caption{Overview of the software development chain}
\end{figure}
\subsection{Automatic code generation}
Figure 2.2 reviews the different steps of this software development chain. We distinguish four main tasks necessary for ensuring an automatic code generation: 

 

\begin{enumerate}
\item \textbf{\textit{Software design:}} 
As part of the generative programming process, the first step consists on representing the system behavior. 
%Software design/behavior is the input program for the code generators. 
On the input side we can either use code as the input or an abstract form that represents the design. It depends on the type of the code generator and on the input source program it requires. These programs can range from a formal specification of the system behavior to abstract models that represents the business logic.
For example, software designers can define, at design time, software’s behavior using for example Domain-Specific Models (DSMs).
A DSM, as an example, is a system of abstractions that describes selected aspects of a sphere of knowledge and real-world concepts pertinent to the domain that need to be modeled in software. These models are specified using a high-level abstract languages (DSLs). %Domain-specific languages (DSLs) improve programmer productivity by providing high-level abstractions for the development of applications in a particular domain. Furthermore, software design can be provided as GUIs, GPLs, Models, etc.

\item \textbf{\textit{Code generation:}} 
Code generation is the technique of building code using programs. The common feature of the generator is to produce code that the software developer would otherwise write by hand.
%There is no one style of code generation. 
Generators are generally seen as a black box which requires as input a program and generate as output a source code for a specific target software platform/language. %Generators can work on the command line or using a GUI. 
Code generation can build code for one or more target language, once or multiple times. There are different varieties of code generation aspects and it highly depends on the type of the input programs described in the previous step. 
%Code generation techniques depends generally on these inputs.  
For example, code generator developers use model-driven engineering techniques in order to automatically generate code. Thus, instead of focusing their efforts on constructing code, they build models and, in particular, create model transformations that transform these models into new models or code. Thus, the code generation process start by taking the previously defined specification to translate a model to an implementation in a target language. We will see in the next section the different types of code generators.


%In general, there are two main categories of Automatic code generation: passive or active.  Passive code generators build the code once, then have nothing more to do with the code.  It is up to the discretion of the user as to how to update and maintain the code.  Active code generators, on the other hand, keep track of the code during its lifecycle.  Active code generators are run on code multiple times during the lifecycle.  With Active Code generators, there is code you can modify, and code that should only be modified by the code generator.  Code generators can be further classified into code mungers, inline code expanders, mixed code generators, partial class generators, tier generators and domain languages\cite{fertalj2008source}. 

\item \textbf{\textit{Software development:}}
Software development may be divided into two main parts. On the one hand, software developers may follow the two previous steps in order to generate automatically code for a specific target software platform. In this case, they use to edit the system specification described in the first step (at a high level) and use to re-generate code each time needed by calling a specific generator. In some cases, generated code can even be edited by the end software developers. This task depends on the complexity of the generated code and it sometimes need software experts that can easily update and maintain the code. However, they may manually implement source code from scratch without using any abstractions or code generation aspects. In this case, they just need to compile and execute the hand-written code in order to test it.

\item \textbf{\textit{Compilation:}}
Once code is generated or implemented, a classical compiler is used to translate the generated code into an executable one. This translation depends on the target hardware platforms and it is up to the software developer to select the adequate compiler to use. Compilers are needed to target heterogeneous and diverse kinds of hardware architectures and devices. 
As an example, cross compilers may be used to create executable code for a platform other than the one on which the compiler is running. In case the generated code needs to run on different machines/devices, the software developer needs to use different compilers for each target software platform and deploy the generated executables within different machines which is a tedious and complicated task.

\end{enumerate} 


\subsection{Stakeholders and their roles for testing generators}
\begin{figure}[h]
	\center
	\includegraphics[scale=0.45]{Background/fig/usecase}
	\caption{Use case diagram of the different actors/roles involved in implementing and testing generators}
\end{figure}
Software development involves several stakeholders that play different roles for validating and testing the software development chain described previously.
Figure 2.7 depicts a use case diagram that describes these different concerns, actors and roles for testing generators.

Basically, we distinguish two stakeholders for code generators and compilers testing: generator user and creator/maintainer. As shown in the bottom of Figure 2.7, creators/maintainers of generators are responsible of the correct functioning of generators. They use their expertise and knowledge associated to the software and hardware technologies, resulting in efficient code generation. They contribute to the software development community by creating and providing new optimizations and compiler versions updates. For code generators, they may use their knowledge to build new platform-specific code generators or enhance existing ones. 

However, users represent the group of software developers that have no knowledge/expertise about the way code is generated. Thus, they are unable to edit or maintain the internal behavior of generators (e.g., the case of commercial and off-the-shell code generators). In this case, generators are used as a black box by engineers during software development to ease code production. Therefore, developers may configure compilers by providing the set of configuration options to efficiently produce code for the target hardware platform (e.g., optimizations options) or maintain/edit the generated code in case of automatic source code generation.

The uses cases highlighted in red in Figure 2.7 constitute the main tasks that we are addressing in this thesis. Our main concern is to evaluate the generated code.

On the one hand, we would help code generator users to automatically generate code for different target software platforms and detect code generator inconsistencies by evaluating the resource usage and performance properties. This task may involve both, code generator users and creators/maintainers.

On the other hand, we would help compiler users to auto-tune compilers through the use of optimizations provided by compiler experts. Similarly, this concerns both actors and it consists in evaluating the impact of these configurations on the performance and resource usage properties and finding the best set of optimizations for a specific program and compiler.


%\begin{figure}[h]
%\center
%	\includegraphics[scale=0.65]{Background/fig/background_overview.pdf}
%	\caption{Overview of the Docker-based testing architecture}
%\end{figure}
\section{Testing code generators}
In this thesis, we focus on testing the automatic code generation process (highlighted in red in the left side of figure 2.6). To do so, we introduce in this section some basis about code generators. We give an overview of the different types of code generators and we discuss their complexity which constitute a major obstacle for testing.
\subsection{Testing workflow}
The main goal of generators is to produce software systems from higher-level specifications. Generators bridge the wide gap between the high-level system description and the executable.
\begin{figure}[h]
	\center
	\includegraphics[scale=1]{Background/fig/workflow}
	\caption{Code generation workflow}
\end{figure}

As stated before, the code generation workflow is divided into two levels. It starts by transforming the system design into source code through the use of generators. Afterwards, source code is transformed into executables using compilers. Thus, software developers use to generate code, edit it (if needed), compile it and then test it. If changes are applied to compilers or generators, the cycle is repeated. Figure 2.7 presents an overview of this testing cycle. The right-hand side of the figure shows the classic workflow for developing and debugging code which is \textit{“edit, compile, and test.”}. The user writes or edits an existing code, compiles it using specific compilers, and tests it. Code generation adds a few new workflow elements in the left-hand side of the figure where generator creators edit the templates and definition files (or the generator itself) and then run the generator to create new output files. The output files are then compiled and the application is tested. 



\subsection{Types of code generators}
There are many ways to categorize generators. We can differentiate them by their complexity, by usage, or by their input/output. According to\cite{herrington2003code}, there are two main categories of automatic code generation: passive or active. Passive code generators build the code only once, then it’s up to the user to update and maintain the code. 
The most common use of passive code generators are wizards. 

Active code generators, run on code multiple times during the lifecycle. With active code generators, there is code can be edited by the users, and code that should only be modified by the code generator. Active code generators are widely referenced in the literature\cite{pais2005tool,amanquah2009rapid}. We focus on this thesis on testing this class code generators.
According to the state-of-the-art\cite{herrington2003code,hunt2000pragmatic,fertalj2008source,bajovs2013code}, there are six categories of active code generators: 

\begin{itemize}
\item \textbf{Code munger:} A code munger reads code as input and then builds new code as output. This new code can either be partial or complete depending on the design of the generator.
A code munger is the most common form of code generators and are used widely. This kind of generators are often used for automatically generating documentations.
A source-to-source compiler, transcompiler or transpiler \footnote{\url{"https://en.wikipedia.org/wiki/Source-to-source_compiler"}} can also be defined as code mungers. A transcompiler takes a code written in some programming language and translates it to a code written in some other language. \textbf{Our contribution related to code generators testing will focus on this kind of generators to validate our approach for automatically detecting inconsistencies.}

Examples:  C2J, JavaDoc, Jazillian, Closure Compiler, Coccinelle, CoffeeScript, Dart, Haxe, TypeScript and Emscripten



\item \textbf{Inline code expander:} This model reads code as input and the builds new code that uses the input code as a base but has sections of the code expanded based on designs in the original code. 
It starts with designing a new language. Usually this new language is an existing language with some syntax extensions. The inline code expander is then used to turn this language into production code in a high-level language.

Example: Embedded SQL languages such as SQLJ (for Java) and Pro*C (for C). The SQL can be embedded in the Cor Java code. The generator builds production C code by expanding the SQL into C code which implements the queries for example.

\item \textbf{Mixed code generator:} This model has the same processing flow as the Inline Code Expander, except that the input file is a real source file that can be compiled and run. The generated output file keep the original markup that will denote where the generated code was placed. It enables code generation for multiple small code fragments within a single file or distributed throughout multiple files. Generally, transformation rules are defined using regular expressions.


Example: Codify is a commercial mixed-code generator which can generate multiple code fragments in a single file from special commands. Another example is the replacement of comments in the input file by the corresponding code.

\item \textbf{Partial class generator:} A partial class generator takes an abstract definition as input instead of code (e.g., UML class diagram) and then builds the output code. User then, can extend it by creating derived classes and extending methods to complete the design. Turning models into code is done through a series of transformations. For example, platform-independent model (PIM) is transformed into a platform specific model (PSM). Then code generation is performed from PSM by using some sort of template-based code transformations.
%As an example, in model-driven engineering, a platform-dependent model (PIM) is transformed into a platform specific model (PSM). PIM to PSM translations are done either by hand or by applying automatic model transformation tools. Then code generation is performed from PSM by using some sort of template-based code generator.

Example: ArgoUML and Codegen translate UML class diagrams to general-purpose languages such as C\#, Java and C++. They do not generate complete implementations, but it tries to convert the input UML class diagrams into skeleton code that the user can easily edit it. 

\item \textbf{Tier generator:} In this model the generator builds a complete set of output code from an abstract definition. It has the same concept as Partial class generator. The big difference between tier and partial class generation is that in the tier model the generator builds all the code for a tier. This code is meant to be used without extension. The partial-class generator model however, lets the engineer create the rest of the derived classes that will complete the functionality for the tier.

Examples: Database Access layer, Web client layer, Data export, import, or conversion layers

\item \textbf{Full-domain language:} Domain languages are basically new languages that have types, syntax and operations and they are used for a specific type of problem. 
Domain languages are the extreme end of automatic code generation because developers have to write a compiler for each problem domain and language. 

Example: Matlab is a domain specific math language that makes it easy to represent mathematical operations for example rather than object-oriented languages. Mathematica, ant and SQL languages are other examples.

\end{itemize}
%Generators are based on domain-specific models which define the semantics of the system specification language and also contain the knowledge of how to produce efficient implementations[REF]. 
%We distinguish two major types of code generators: rule-based model-to-model transformation languages (such as ATL) and template-based model-to-text transformation languages (such as Acceleo) to translate high-level system specifications into executable code and scripts.
\subsection{Why testing code generators is complex?}
%Code generation deals with the partial or complete generation of programs, based on a formal model• A model could be written in a specific language (model language), existing source code or also available meta information (Database Metadata, XML-Schema, ...)• Regular Expressions are a powerful language to extract information from code or a formal model• Lightweight software generators consist often only about a dozens of lines• Code generation yields to higher abstraction, higher productivity, improved quality and a higher consistence of your application


%A model could be written in a specific language (model language), existing source code or also available meta information (Database Metadata, XML-Schema, ...)• Regular Expressions are a powerful language to extract information from code or a formal model• Lightweight software generators consist often only about a dozens of lines• Code generation yields to higher abstraction, higher productivity, improved quality and a higher consistence of your application

The complexity of code generators remains on the different code generation models described above.
In fact, code generators can be difficult to understand since they are typically composed of numerous elements, whose complex interdependencies pose important challenges for developers performing design, implementation, and maintenance tasks. 
Given the complexity and heterogeneity of the technologies involved in a code generator, developers who are trying to inspect and understand the code-generation process have to deal with numerous different artifacts. As an example, in a code-generator maintenance scenario, a developer might need to find all chained model-to-model and model-to-text transformation bindings, that originate a buggy line of code to fix it. This task is error prone when done manually. We believe that flexible traceability tools are needed to collect and visualize information about the architecture and operational mechanics of code generators, to reduce the challenges that developers face during their life-cycle\cite{guana2015developers}.

Moreover, the generated code has to meet certain performance requirements (e.g. execution speed, response time, memory consumption, utilization of resources, etc.). The challenge is that the structure of the specification is usually very different from the structure of the implementation: there is no simple one-to-one correspondence between the concepts in the specification and the concepts in the implementation. 
%Efficient implementations are then computed at generation time by applying domain-specific optimizations and replacing, merging, adding, and removing components.

%\begin{figure}[h]
%	\center
%	\includegraphics[scale=0.65]{Background/fig/software-diversity.pdf}
%	\caption{Code generation process}
%\end{figure}




\section{Compilers auto-tuning}
The compiler is a very essential software component in software engineering, responsible for translating user'source code written in general purpose languages into machine code. The key feature of compilers is to bridge source programs written in high-level languages with the underlying hardware architecture.

High-level languages are used to help the software developer to have an easier and simpler way for writing programs. They offer many abstract programming features such as functions, data structures, conditional statements and loops that facilitates software development.
Writing code in a high-level programming language may induce significant decrease in performance. Principally, software developers should write understandable, maintainable code without putting too much emphasizes on the performance for example. 

This means that the compiler has a major role in producing fast and efficient target machine code automatically. This is not a trivial task because potentially many variants of the machine code exist for the same program. Hence, the task of the compiler is to find and produce the best version of the machine code for any given program. For this reason, compilers generally attempt to automatically optimize the code to improve its performance.

This process is called program optimization. 
\subsection{Code optimizations}

Code optimization within a compiler is the process of transforming a source code program into another functionally equivalent code for the purpose of improving one or more of its non-functional properties. 

The most common outcome of optimizations is to minimize the execution time of program execution. Other less common non-functional properties are code size, memory usage and power consumption. 

There exist many types of optimizations such as loop unrolling, automatic parallelization, code-block reordering and functions inlining among others. The factors that affect optimizations may include characteristics such as: the number of CPU registers (the more registers, the easier it is to optimize for performance), cache size, CPU architecture, etc.

Optimization can be categorized broadly into two types: machine independent and machine dependent: 
\begin{itemize}
	
	\item \textbf{Machine-independent optimization:}
	
	Intermediate code generation process introduces many inefficiencies such as extra copies of variables and using variables instead of
	constants.
	This optimization removes such inefficiencies and improves code. Thus, the compiler takes in the intermediate code and transforms a part of the code regardless of any CPU registers or memory locations. These optimizations generally change the structure of programs.
	Optimizations that are applied on abstract programming concepts (structures, loops, objects, functions) are independent of the machine targeted by the compiler.
	
	\textbf{Example:} Eliminate redundancy, loop unrolling, eliminate useless and unreachable code, function inlining, dead-code elimination, etc.
	
	\item \textbf{Machine-dependent optimization:} 
	Machine-dependent optimizations are applied after generating the target code and when the code is transformed according to the target machine architecture. They take advantage of special hardware features to produce code which is shorter or which executes more quickly on the machine such as instruction selection, register allocation, instruction scheduling, introduce parallelism, etc.
	They mostly involve CPU registers and memory references. Machine-dependent optimizers put efforts to take maximum advantage of memory hierarchy. They are more effective and have better impact on performance than independent optimizations because they best exploit special features of the target platform.
	
	\textbf{Example:} Register allocation optimizations for efficient utilization of registers, branch prediction, loop optimization, etc
 
 
\end{itemize}
 

 
 
 

\subsection{Why compilers auto-tuning is complex?}

%\subsection{Why complier auto-tuning is complex?}
Today, modern compilers implement a broad number of optimizations. Each optimization tries to improve the performance of the input application.

On the one hand, optimizing compilers becomes quite sophisticated nowadays and creating compiler optimizations for a new microprocessor is a hard and time-consuming work because it requires a comprehensive understanding of the underlying hardware architecture as well as an efficient way to evaluate the optimization impact on performance and resource usage. 

On the other hand and from the compiler user perspective, applying and evaluating optimizations is challenging because the determination of optimal settings of compiler optimizations has been identified as a major problem\cite{knijnenburg2002iterative}.

We resume, in the following, several issues with optimizing compiler technology which make the activity of compiler tuning very complex:

\begin{itemize}
	\item[--] \textbf{Conflicting objectives:} Compilers usually have to support a variety of conflicting objectives, such as execution time, compilation speed, resource usage and quality of generated code. It is difficult to define a set of optimizations that satisfy all properties.
	
	\item[--] \textbf{Optimization interactions:} The interaction between optimization phases as well their application order make it difficult to find an optimal sequence.
	
	\item[--] \textbf{Huge number of optimizations:} The huge number of optimizations is also an issue for the compiler user to choose the best optimization sequence since an exhaustive search is impossible (we count $2^{number\ of\ optimizations}$ possible combination to evaluate).
	
	\item[--] \textbf{Non universal optimizations:} There is no one universal optimization sequence that will enhance the performance of all programs. Optimization's impact depends on the hardware and on the input program. Thus, constructing an optimization sequence for different programs and hardware architectures becomes very hard and time-consuming.
	
	\item[--] \textbf{Compiler bugs:} Optimizations may lead to compiler bug and introduce errors in the compiled code. Optimizations must not cause any change in program behavior under any possible condition\cite{le2014compiler,yang2011finding}.
	
	\item[--] \textbf{Optimization overhead:} Optimizations should be fast and efficient. They should not delay the overall compiling process.
	
	\item[--] \textbf{Tuning compilers need expertise:} In case the compiler user has no knowledge and expertise about the compiler technology and its optimizations, it will be quite hard to select the set of optimization sequences to apply.
\end{itemize}

 

%Improvement of source code programs in terms of performance can refer to several different non-functional properties of the produced code such as code size, resource or energy consumption, execution time, among others~\cite{almagor2004finding,pan2006fast}.
%\begin{figure}[h]
%	\center
%	\includegraphics[scale=0.65]{Background/fig/hardware-diversity.pdf}
%	\caption{Compiler optimizations}
%\end{figure}


\section{Summary: Testing challenges}

\begin{itemize}
	\item \textbf{Auto-tuning compilers}: 
	Compilers may have a huge number of potential optimization combinations, making it hard and time-consuming for software developers to find/construct the sequence of optimizations that satisfies user specific key objectives and criteria. It also requires a comprehensive understanding of the available optimizations of the compiler.  
	So, how can we help the compiler user to automatically auto-tune compilers and choose which optimizations he should apply to satisfy a specific non-functional property? 
	
		
	\item \textbf{Detecting code generator inconsistencies}: 
    Automatic code generation offers many gains over traditional software development methods. e.g., speed of development, increased adaptability and reliability. But code generators are complex pieces of software that may themselves contain bugs. Thus, testing code generators becomes very needed.
	So, how can we automatically detect issues within code generators? 
	Moreover, proving that the generated code is functionally correct is not enough to claim the effectiveness of the code generator under test?
	What about testing the non-functional properties of automatically generated code?
	%can we trust the code-generator?
%	How can the correctness of the generated code be verified?
	
	
	
	\item \textbf{Resource usage monitoring of generated code:} Analyzing the resource usage of optimized or generated code requires a dynamic and adaptive solution that extract efficiently those properties. Due to the software diversity and hardware heterogeneity, monitoring the resource usage of each execution platform becomes challenging and time-consuming. So, how can we ease this process and provide an efficient solution that will help compiler users/experts to evaluate the optimizations and code generator users/experts to test the generated code in terms of non-functional properties?
	

%\subsection{Resource usage monitoring}
\end{itemize}


%
